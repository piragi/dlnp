{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all the input data has to be lowercase for the embeddings, it does not recoginize \"The\"\n",
    "- only 20 unique sentences, they go in a loop.\n",
    "- construct your own dataset from some book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words with 300-dimensional embeddings.\n"
     ]
    }
   ],
   "source": [
    "# download embeddings:\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "def load_glove_embeddings(dim):\n",
    "    # Loads the specified GloVe embeddings\n",
    "    glove = vocab.GloVe(name='6B', dim=dim)\n",
    "    return glove\n",
    "\n",
    "# Specify the desired dimension\n",
    "embed_dim = 300  # For 100-dimensional embeddings\n",
    "glove_embeddings = load_glove_embeddings(embed_dim)\n",
    "\n",
    "# Now, `glove_embeddings` holds the loaded embeddings\n",
    "print(f\"Loaded {len(glove_embeddings.stoi)} words with {embed_dim}-dimensional embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/piragi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Ensure the punkt tokenizer is downloaded\n",
    "\n",
    "def create_dataset_from_file(input_file, output_file, window_size=9, step_size=1):\n",
    "    # Read the text from file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Generate data points\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > window_size:\n",
    "            for i in range(0, len(words) - window_size, step_size):\n",
    "                current_sequence = \" \".join(words[i:i + window_size])\n",
    "                next_word = words[i + window_size]\n",
    "                data.append({\"sentence\": current_sequence, \"next_word\": next_word})\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump({\"data\": data}, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Set the path to your .txt file and the output file\n",
    "input_file_path = './frankenstein.txt'\n",
    "output_file_path = './frankenstein_dataset.json'\n",
    "\n",
    "# Create the dataset\n",
    "create_dataset_from_file(input_file_path, output_file_path, window_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_word\n",
      "the      3035\n",
      "and      2335\n",
      "of       2195\n",
      "to       1664\n",
      "I        1515\n",
      "my       1252\n",
      "a        1001\n",
      "in        839\n",
      "that      752\n",
      "was       551\n",
      "with      493\n",
      "which     480\n",
      "had       457\n",
      "but       453\n",
      "his       428\n",
      "me        405\n",
      "as        372\n",
      "for       367\n",
      "by        349\n",
      "he        333\n",
      "on        327\n",
      "from      314\n",
      "not       312\n",
      "it        274\n",
      "you       256\n",
      "Name: count, dtype: int64\n",
      "next_word\n",
      "the      5.590451\n",
      "and      4.301055\n",
      "of       4.043176\n",
      "to       3.065078\n",
      "I        2.790621\n",
      "my       2.306176\n",
      "a        1.843836\n",
      "in       1.545433\n",
      "that     1.385179\n",
      "was      1.014939\n",
      "with     0.908103\n",
      "which    0.884157\n",
      "had      0.841791\n",
      "but      0.834423\n",
      "his      0.788373\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "with open('./frankenstein_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data['data'])\n",
    "\n",
    "# Get the distribution of 'next_word'\n",
    "next_word_distribution = df['next_word'].value_counts()\n",
    "\n",
    "# Print the distribution\n",
    "print(next_word_distribution.head(25))\n",
    "\n",
    "# If you want to see the distribution in percentage\n",
    "next_word_percentage = df['next_word'].value_counts(normalize=True) * 100\n",
    "print(next_word_percentage.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Enhanced dataset with more variety\n",
    "sentences = [\n",
    "    \"The cat sat on the\", \"The dog ran around the\", \"The sun was shining in the\",\n",
    "    \"The baby laughed at the\", \"The teacher wrote on the\", \"A car drove over the\",\n",
    "    \"He opened the\", \"She closed the\", \"The player won the\", \"The artist drew a\",\n",
    "    \"The girl danced in the\", \"The boy played in the\", \"The wind blew through the\",\n",
    "    \"The author discussed the\", \"The scientist discovered a\", \"The historian studied the\",\n",
    "    \"The chef cooked a\", \"The farmer planted a\", \"The journalist wrote about the\",\n",
    "    \"The programmer debugged the\"\n",
    "]\n",
    "next_words = [\n",
    "    \"mat\", \"block\", \"sky\", \"toy\", \"board\", \"bridge\", \"door\", \"window\", \"game\", \"portrait\",\n",
    "    \"room\", \"yard\", \"trees\", \"topic\", \"method\", \"artifact\", \"dish\", \"seed\", \"event\", \"program\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"next_word\": next_words})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "# Load GloVe Embeddings\n",
    "glove_file = \"./.vector_cache/glove.6B.300d.txt\"\n",
    "glove_embeddings = {}\n",
    "with open(glove_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor([float(x) for x in values[1:]], dtype=torch.float32)\n",
    "        glove_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_json = pd.read_json('./frankenstein_dataset.json')\n",
    "# turn into dataframe make the right columns\n",
    "df = pd.DataFrame( columns=['sentence', 'next_word'])\n",
    "for i in range(len(df_json['data'])):\n",
    "    # no append\n",
    "    df.loc[i] = [df_json['data'][i]['sentence'].lower(), df_json['data'][i]['next_word'].lower()]\n",
    "'''\n",
    "This is only an example. Change as you see fit. Right now there is not split between train and test data. You will have to implement that.\n",
    "'''    \n",
    "# Assuming glove_embeddings has been loaded as shown earlier\n",
    "def sentence_to_embedding(sentence):\n",
    "    words = sentence.split()\n",
    "    embeddings = [glove_embeddings.get(word, torch.zeros(300, dtype=torch.float32)) for word in words]\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        next_words = df[\"next_word\"].tolist()\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(sorted(set(next_words)))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.df.iloc[idx, 0] \n",
    "        next_word = self.df.iloc[idx, 1] \n",
    "        embedding = sentence_to_embedding(sentence)\n",
    "        next_word_idx = self.word_to_idx[next_word]\n",
    "        return {\"sentence\": embedding, \"next_word\": next_word_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data randomly\n",
    "# split into 0.7, 0.15, 0.15\n",
    "# evaluate using k-fold cross evaluation\n",
    "\n",
    "#randomized_df = df.sample(frac=1, random_state=69).reset_index(drop=True)\n",
    "data_len = len(df_json['data'])\n",
    "train_split = int(data_len * 0.7)\n",
    "test_split = train_split + int(data_len * 0.15)\n",
    "\n",
    "train_df = df.iloc[:train_split]\n",
    "test_df = df.iloc[train_split:test_split]\n",
    "val_df = df.iloc[test_split:]\n",
    "\n",
    "train_dataset = SentenceDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = SentenceDataset(test_df)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = SentenceDataset(val_df)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its\n",
      "                                 sentence next_word\n",
      "21508  edinburgh, its romantic castle and       its\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3251,  0.0184, -0.0720,  ..., -0.1204, -0.2595, -0.4693],\n",
      "        [-0.2959, -0.7609,  0.1986,  ...,  0.3912, -0.1467,  0.0756],\n",
      "        [-0.1766, -0.1067,  0.0927,  ...,  0.6462, -0.2600,  0.5391],\n",
      "        [ 0.0385, -0.0398,  0.0827,  ..., -0.3343,  0.0118,  0.0597]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3251,  0.0184, -0.0720,  ..., -0.1204, -0.2595, -0.4693],\n",
      "        [-0.2959, -0.7609,  0.1986,  ...,  0.3912, -0.1467,  0.0756],\n",
      "        [-0.1766, -0.1067,  0.0927,  ...,  0.6462, -0.2600,  0.5391],\n",
      "        [ 0.0385, -0.0398,  0.0827,  ..., -0.3343,  0.0118,  0.0597]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence_to_embedding(test_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sentence_to_embedding(test_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]), test_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_word\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "print(test_df.iloc[0, 1])\n",
    "print(test_df.head(1))\n",
    "print(test_dataset[0]['sentence'])\n",
    "print(sentence_to_embedding(test_df.iloc[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2084, Perplexity: 1.2317, Test Loss: 0.3138, Test Accuracy: 0.02% Training Accuracy: 10.99%\n",
      "Epoch 2, Loss: 0.1791, Perplexity: 1.1961, Test Loss: 0.3419, Test Accuracy: 0.04% Training Accuracy: 14.58%\n",
      "Epoch 3, Loss: 0.1547, Perplexity: 1.1673, Test Loss: 0.3677, Test Accuracy: 0.04% Training Accuracy: 16.80%\n",
      "Epoch 4, Loss: 0.1283, Perplexity: 1.1369, Test Loss: 0.4071, Test Accuracy: 0.02% Training Accuracy: 21.76%\n",
      "Epoch 5, Loss: 0.1026, Perplexity: 1.1080, Test Loss: 0.4464, Test Accuracy: 0.02% Training Accuracy: 30.57%\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # Save hidden_dim as an instance variable\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=1)\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        # Initialize hidden and cell states with dimensions: (num_layers, batch_size, hidden_dim)\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(1, x.size(0), self.hidden_dim)\n",
    "            c0 = torch.zeros(1, x.size(0), self.hidden_dim)\n",
    "        # Forward pass through LSTM layer\n",
    "        out, (hn,cn) = self.lstm(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, (hn,cn)\n",
    "\n",
    "\n",
    "output_dim = len(train_dataset.word_to_idx)  # Number of unique next words\n",
    "model = LSTMModel(input_dim=300, hidden_dim=128, output_dim=output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(loss)\n",
    "\n",
    "# Training Loop with Perplexity Calculation\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    train_accuracy = 0\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        sentences = batch[\"sentence\"]\n",
    "        next_words = batch[\"next_word\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(sentences)\n",
    "        loss = criterion(outputs, next_words)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        accuracy = 0\n",
    "        for prediction, target in zip(outputs.argmax(1), next_words):\n",
    "            if prediction == target:\n",
    "                accuracy += 1\n",
    "        train_accuracy += accuracy\n",
    "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
    "    train_accuracy = (train_accuracy / len(train_dataloader.dataset)) * 100\n",
    "    perplexity = calculate_perplexity(torch.tensor(avg_loss))\n",
    "\n",
    "    model.eval()\n",
    "    test_total_loss = 0\n",
    "    test_accuracy = 0\n",
    "    for test_batch in test_dataloader:\n",
    "        sentences = test_batch[\"sentence\"]\n",
    "        next_words = test_batch[\"next_word\"]\n",
    "        outputs, _ = model(sentences)\n",
    "        loss = criterion(outputs, next_words)\n",
    "        test_total_loss += loss.item()\n",
    "        accuracy = 0\n",
    "        for prediction, target in zip(outputs.argmax(1), next_words):\n",
    "            if prediction == target:\n",
    "                accuracy += 1\n",
    "        test_accuracy += accuracy\n",
    "    test_avg_loss = test_total_loss / len(test_dataloader.dataset)\n",
    "    test_accuracy = (test_accuracy / len(test_dataloader.dataset)) * 100\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Perplexity: {perplexity.item():.4f}, Test Loss: {test_avg_loss:.4f}, Test Accuracy: {test_accuracy:.2f}% Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.40068811604984195\n"
     ]
    }
   ],
   "source": [
    "train_total_loss = 0\n",
    "train_accuracy = 0\n",
    "for train_batch in test_dataloader:\n",
    "    accuracy = 0\n",
    "    sentences = train_batch[\"sentence\"]\n",
    "    next_words = train_batch[\"next_word\"]\n",
    "    outputs, _ = model(sentences)\n",
    "    loss = criterion(outputs, next_words)\n",
    "    train_total_loss += loss.item()\n",
    "    for prediction, target in zip(outputs.argmax(1), next_words):\n",
    "        if prediction == target:\n",
    "            accuracy += 1\n",
    "    train_accuracy += accuracy\n",
    "print(f'Training Accuracy: {train_accuracy / len(train_dataloader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.40068811604984195\n"
     ]
    }
   ],
   "source": [
    "train_total_loss = 0\n",
    "train_accuracy = 0\n",
    "for train_batch in test_dataloader:\n",
    "    accuracy = 0\n",
    "    sentences = train_batch[\"sentence\"]\n",
    "    next_words = train_batch[\"next_word\"]\n",
    "    outputs, _ = model(sentences)\n",
    "    loss = criterion(outputs, next_words)\n",
    "    train_total_loss += loss.item()\n",
    "    for prediction, target in zip(outputs.argmax(1), next_words):\n",
    "        if prediction == target:\n",
    "            accuracy += 1\n",
    "    train_accuracy += accuracy\n",
    "print(f'Training Accuracy: {train_accuracy / len(train_dataloader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "correct 1 out of 577\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_batch in val_dataloader:\n",
    "    model.eval()\n",
    "    sentences = test_batch[\"sentence\"]\n",
    "    next_words = test_batch[\"next_word\"]\n",
    "    outputs, _ = model(sentences)\n",
    "    loss = criterion(outputs, next_words)\n",
    "    test_total_loss += loss.item()\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i].argmax() == next_words[i]:\n",
    "            print(list(train_dataset.word_to_idx.keys())[next_words[i]])\n",
    "    correct += (outputs.argmax(1) == next_words).sum().item()\n",
    "\n",
    "\n",
    "print(f'correct {correct} out of {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat sat on the table,\n"
     ]
    }
   ],
   "source": [
    "# check the model\n",
    "model.eval()\n",
    "sentence = \"the cat sat on the \"\n",
    "sentence_embedding = sentence_to_embedding(sentence).unsqueeze(0)\n",
    "output, _ = model(sentence_embedding)\n",
    "topk_values, topk_indices = torch.topk(output, 5, dim=1)\n",
    "predicted_words = [list(train_dataset.word_to_idx.keys())[idx.item()] for idx in topk_indices[0]]\n",
    "random_index = torch.randint(0, 5, (1,)).item()\n",
    "sentence = sentence + predicted_words[random_index]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: The cat sat on the heart, over the chairs, perish that ten where i mentioned the are of henry? but there is only all the dreadful vast sensations on the summit he left that departed. which i had at some \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "def generate_sentence(model, start_sentence, max_length=35):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    sentence = start_sentence\n",
    "    for _ in range(max_length):\n",
    "        output, _ = model(sentence_to_embedding(sentence).unsqueeze(0))\n",
    "        _, predicted_idx = torch.topk(output, 4, dim=1)\n",
    "        random_index = torch.randint(0, 4, (1,)).item()\n",
    "        predicted_idx = predicted_idx[0][random_index]\n",
    "        predicted_word = list(train_dataset.word_to_idx.keys())[predicted_idx.item()]\n",
    "        sentence += predicted_word + ' '\n",
    "\n",
    "    words = sentence.split()\n",
    "    return sentence\n",
    "\n",
    "# Example usage after training\n",
    "start_fragment = \"The cat sat on the \"\n",
    "generated_sentence = generate_sentence(model, start_fragment)\n",
    "#used_words = set(words)  # Keep track of words used in the sentence to apply penalties\n",
    "print(\"Generated Sentence:\", generated_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to mitigate repeating words.\n",
    "change the hidden state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded glove embedding\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(path, embedding_dim=50):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        vocab, vectors = {}, []\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = [float(x) for x in parts[1:]]\n",
    "            vocab[word] = len(vectors)\n",
    "            vectors.append(vector)\n",
    "        return vocab, torch.tensor(vectors, dtype=torch.float32)\n",
    "\n",
    "glove_vocab, glove_vectors = load_glove_embeddings('.vector_cache/glove.6B.50d.txt')\n",
    "print('loaded glove embedding')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, glove_vocab, glove_vectors):\n",
    "        self.data = data\n",
    "        self.glove_vocab = glove_vocab\n",
    "        self.glove_vectors = glove_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, next_word = self.data[idx]['sentence'], self.data[idx]['next_word']\n",
    "        # Convert sentence to indices\n",
    "        indices = [self.glove_vocab[word] if word in self.glove_vocab else 0 for word in sentence.split()]\n",
    "        # Convert next_word to index\n",
    "        next_word_idx = self.glove_vocab.get(next_word, 0)  # using 0 for unknown words\n",
    "        return torch.tensor(indices), next_word_idx\n",
    "\n",
    "# Assuming `data` is loaded as shown in your JSON structure example\n",
    "train_data, test_data = train_test_split(data['data'], test_size=0.3, random_state=420)\n",
    "train_dataset = TextDataset(train_data, glove_vocab, glove_vectors)\n",
    "test_dataset = TextDataset(test_data, glove_vocab, glove_vectors)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe embeddings\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# Function to load GloVe embeddings\n",
    "def load_glove_embeddings(path, embedding_dim=50):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        vocab, vectors = {}, []\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = [float(x) for x in parts[1:]]\n",
    "            vocab[word] = len(vectors)\n",
    "            vectors.append(vector)\n",
    "        return vocab, torch.tensor(vectors, dtype=torch.float32)\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_vocab, glove_vectors = load_glove_embeddings('.vector_cache/glove.6B.50d.txt')\n",
    "print('Loaded GloVe embeddings')\n",
    "\n",
    "# Define the custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, glove_vocab):\n",
    "        self.data = data\n",
    "        self.glove_vocab = glove_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, next_word = self.data[idx]['sentence'], self.data[idx]['next_word']\n",
    "        # Convert sentence to indices based on GloVe vocabulary\n",
    "        indices = [self.glove_vocab.get(word.lower(), 0) for word in sentence.split()]  # Unknown words are indexed as 0\n",
    "        # Convert next_word to its index, using 0 for unknown words\n",
    "        next_word_idx = self.glove_vocab.get(next_word.lower(), 0)\n",
    "        return torch.tensor(indices, dtype=torch.long), next_word_idx\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('frankenstein_dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(data['data'], test_size=0.3, random_state=69)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_data, glove_vocab)\n",
    "test_dataset = TextDataset(test_data, glove_vocab)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence indices: tensor([ 4838,    44,  1454,     4, 24507,     0,  2677]), Next word index: 53714\n",
      "Sentence indices: tensor([   0,    0,   14,   56, 3958,    3,  392]), Next word index: 8625\n",
      "Sentence indices: tensor([   19,     0,  4821,     0,    41, 23672,    75]), Next word index: 7\n",
      "Sentence indices: tensor([   0,   42,   41,   40, 1435,    3,  151]), Next word index: 6\n",
      "Sentence indices: tensor([  67,   40,  227,    0, 1058,    3,    0]), Next word index: 629\n",
      "Sentence indices: tensor([   41,   238,    12,     0, 19740,    35, 16265]), Next word index: 21\n",
      "Sentence indices: tensor([    22,      0,   3453,   2641,      0,  71977, 122900]), Next word index: 12\n",
      "Sentence indices: tensor([   40, 44416,     0,     5,     0,   215,     0]), Next word index: 4\n",
      "Sentence indices: tensor([ 2025,   249,     5,  5173,     6, 27388,     5]), Next word index: 0\n",
      "Sentence indices: tensor([ 26,   0,  18, 822,   7,   0,  17]), Next word index: 7\n",
      "Sentence indices: tensor([103,  19,  18,  16,   0,  26, 621]), Next word index: 15\n",
      "Sentence indices: tensor([  573,    55,   265,    33, 36548,   108,     0]), Next word index: 3737\n",
      "Sentence indices: tensor([   0, 6575, 1721,    3,    0, 1159,   15]), Next word index: 29\n",
      "Sentence indices: tensor([ 2486,     3,     0,   192, 76957,   399,    56]), Next word index: 23673\n",
      "Sentence indices: tensor([   26,     0,     0,    41,  4172,     0, 27625]), Next word index: 106\n"
     ]
    }
   ],
   "source": [
    "# Fetch the first batch from the DataLoader\n",
    "first_batch_indices, first_batch_next_word_idxs = next(iter(test_loader))\n",
    "\n",
    "# Display the first five entries from the first batch\n",
    "for i in range(15):\n",
    "    indices = first_batch_indices[i]\n",
    "    next_word_idx = first_batch_next_word_idxs[i]\n",
    "    print(f\"Sentence indices: {indices}, Next word index: {next_word_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.5213, Test Loss: 5.9896, Test Accuracy: 0.2001\n",
      "Epoch 2, Train Loss: 5.6480, Test Loss: 5.9448, Test Accuracy: 0.2001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     54\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 55\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     58\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/env/lib/python3.10/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=2, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        logits = self.fc(lstm_out[:, -1, :])  # We only use the output of the last time step\n",
    "        return logits\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(50, 256, len(glove_vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, targets in test_loader:\n",
    "            sentences, targets = sentences.to(device), targets.to(device)\n",
    "            outputs = model(sentences.long())\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training and evaluation loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for sentences, targets in train_loader:\n",
    "        sentences, targets = sentences.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sentences.long())\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  5, 34, 41, 42]])\n",
      "tensor([[ 0,  5, 41, 34, 42]])\n",
      "tensor([[41, 40, 18, 20, 15]])\n",
      "tensor([[ 40,   0, 913,  15, 189]])\n",
      "tensor([[33,  0, 36, 30, 41]])\n",
      "tensor([[ 0, 34,  5, 41, 18]])\n",
      "tensor([[ 0, 41,  5, 34, 42]])\n",
      "tensor([[ 40,   0,  15, 913, 189]])\n",
      "tensor([[ 0,  4, 40, 36, 33]])\n",
      "tensor([[  0,  51,  36, 192,  40]])\n",
      "tensor([[   0,  261,  629,    7, 1773]])\n",
      "tensor([[ 0, 40, 15,  5, 14]])\n",
      "tensor([[  0,  41, 192,   7,  12]])\n",
      "tensor([[ 40,   0,  15, 913,  33]])\n",
      "tensor([[  0,  36, 114, 192,   6]])\n",
      "tensor([[ 0,  5, 34, 41,  4]])\n",
      "tensor([[ 40,   0,  15, 913, 189]])\n",
      "tensor([[   0,   36,  114,    6, 2282]])\n",
      "tensor([[ 4,  0,  6,  7, 17]])\n",
      "tensor([[ 5,  0, 34, 41,  4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the cat sat on the the which i should the the i am have my father and i was the i was now the i '"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import random as rand\n",
    "\n",
    "def embed_sentence(sentence, glove_vocab): return [glove_vocab.get(word.lower(), 0) for word in sentence.split()] \n",
    "def deembed_sentence(indices, glove_vocab): return [list(glove_vocab.keys())[idx] for idx in indices]\n",
    "def generate_sentence(model, start_sentence, max_length=20):\n",
    "    model.eval()\n",
    "    for _ in range(max_length):\n",
    "        embedded_sentence = torch.tensor(embed_sentence(start_sentence, glove_vocab)).unsqueeze(0)\n",
    "        embedded_sentence.to(device)\n",
    "        outputs = model(embedded_sentence)\n",
    "        _, predicted_idx = torch.topk(outputs, 5, dim=1)\n",
    "        print(predicted_idx)\n",
    "        random_index = rand.randint(0, 4)\n",
    "        predicted_idx = predicted_idx[0][random_index]\n",
    "        predicted_word = list(glove_vocab.keys())[predicted_idx.item()]\n",
    "        start_sentence += predicted_word + ' '\n",
    "    return start_sentence\n",
    "\n",
    "sentence = \"the cat sat on the \"\n",
    "generate_sentence(model, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
