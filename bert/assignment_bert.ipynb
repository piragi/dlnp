{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8_wEtBmL1Hwk"
      },
      "source": [
        "# Assignment - Transformer and BERT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OTZtiOeg1vHP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lL9OYI1C1wRq"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QilVSubqFXn4"
      },
      "source": [
        "## Transformer and BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RBrRyCdUFhlj"
      },
      "source": [
        "In this assignment, you will:\n",
        "- Implement a simplified BERT from scratch\n",
        "- Visualize attention in your implementd model\n",
        "- Fine-tune a pre-trained BERT model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gVQ9Q4lw4_XA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/piragi/projects/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g7xU1WDCtLvd"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyRw05ButO-4"
      },
      "source": [
        "In order to implement BERT, we should first implement the encoder layer of the transformer. An encoder has 2 main sub-layers: multi-headed attention layer and a simple feed-forward layer. The multi-headed attention layer is already implemented , but you should implement the feedforward sub-layer \n",
        "<br>\n",
        "<center>\n",
        "\n",
        "![](https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/encoder.PNG)\n",
        "\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "prl-PotP4gNl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ------------------------------ Encoder ------------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.projection_dim = hidden_size // num_heads\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        # print(f'Query shape before reshape: {query.shape}')\n",
        "        # print(f'Key shape before reshape: {key.shape}')\n",
        "\n",
        "        query = query.reshape(query.shape[0], query.shape[1], -1, self.projection_dim)\n",
        "        key = key.reshape(key.shape[0], key.shape[1], -1, self.projection_dim)\n",
        "        value = value.reshape(value.shape[0], value.shape[1], -1, self.projection_dim)\n",
        "\n",
        "        score = torch.matmul(query, key.transpose(-2, -1))\n",
        "        dim_key = torch.tensor(key.shape[-1], dtype=torch.float32)\n",
        "        scaled_score = score / torch.sqrt(dim_key)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Change from mask.unsqueeze(1) to mask.unsqueeze(1).unsqueeze(2)\n",
        "            # to add an extra dimension for the heads and then expand to match the score dimensions\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)  # Add a new dimension for heads\n",
        "            mask = mask.expand_as(scaled_score)  # Ensure mask is correctly expanded\n",
        "            masked_score = scaled_score.masked_fill(mask == 0, -1e9)\n",
        "        else:\n",
        "            masked_score = scaled_score\n",
        "\n",
        "        weights = F.softmax(masked_score, dim=-1)\n",
        "        output = torch.matmul(weights, value)\n",
        "\n",
        "        #print(f'Output shape after matmul: {output.shape}')\n",
        "        return output, weights\n",
        "\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = x.reshape(batch_size, -1, self.num_heads, self.projection_dim)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, inputs, att_mask):\n",
        "        batch_size = inputs.shape[0]\n",
        "        query = self.separate_heads(self.Q(inputs), batch_size)\n",
        "        key = self.separate_heads(self.K(inputs), batch_size)\n",
        "        value = self.separate_heads(self.V(inputs), batch_size)\n",
        "\n",
        "        # print(f'Separate heads shapes - Query: {query.shape}, Key: {key.shape}, Value: {value.shape}')\n",
        "\n",
        "        attention, self.att_weights = self.attention(query, key, value, att_mask)\n",
        "        attention = attention.permute(0, 2, 1, 3)\n",
        "\n",
        "        # print(f'Attention shape after permute: {attention.shape}')\n",
        "\n",
        "        concat_attention = attention.reshape(batch_size, -1, self.hidden_size)\n",
        "\n",
        "        # print(f'Concat attention shape: {concat_attention.shape}')\n",
        "\n",
        "        output = self.out(concat_attention)\n",
        "\n",
        "        # print(f'Output shape: {output.shape}')\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "558gGbanoBsu"
      },
      "source": [
        "**Question**: Why does the transformer use multi-headed attention instead of just a single self-attention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r4DkUpBToQ8v"
      },
      "source": [
        "<font color=red> Write your answer here</font>\n",
        "Multiple heads can represent attention for different layers of context. Thinking about NLP, one head would be responsible about which word is semantically connected to the other. Another head would layer the grammatical interplay between these words. The more layers the more potential representability of hidden latent knowledge inbetween the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KCVLJBv951nW"
      },
      "source": [
        "#### Feed-Forward Sub-Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FRnFXb0WwwSB"
      },
      "source": [
        "The feed-forward sub-layer of the encoder has two dense layers. The first dense layer is called the \"intermediate\" layer and the second one is the \"output\" layer whose functionality is to down-project back to the hidden layer size. Dropout is also applied to the output of the intermediate layer. Unlike the original transformer, BERT uses \"GELU\" activation function in the intermediate dense layer. Since there is no GELU activation function in TensorFlow (there is one in TensorFlow Addons but it will crash your session!), you should implement it yourself!\n",
        "\n",
        "Here is the GELU paper: https://arxiv.org/abs/1606.08415 . Or you can just search the internet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qzD7BjELQ--j"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def GELU(x):\n",
        "  constant = 0.044715\n",
        "  return 0.5*x * (1+torch.tanh(math.sqrt(2/math.pi)*(x+(constant*x**3))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Gqd6wedZXxzD"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, intermediate_size, hidden_size, drop_rate):\n",
        "        super(FFN, self).__init__()\n",
        "        self.intermediate = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.out = nn.Linear(intermediate_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.intermediate(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x = GELU(x)\n",
        "        x = self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aDlwb3Ea6Aqc"
      },
      "source": [
        "#### Residual Connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K4-UMLpDUkFa"
      },
      "source": [
        "In the encoder, dropout is applied to each sub-layer's output, then it gets added to the sub-layer's input (residual connection) and finaly goes through a layer normalizaion step. You should implement all the aforementioned steps in the **AddNorm** custom layer in the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_TtnesNMOHUF"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, LNepsilon, drop_rate):  # Add hidden_size\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.LN = nn.LayerNorm(hidden_size, eps=LNepsilon)  # Correctly use hidden_size\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, sub_layer_in, sub_layer_out):\n",
        "          x = self.dropout(sub_layer_out)\n",
        "          x = sub_layer_in + x\n",
        "          x = self.LN(x)\n",
        "          return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AKqyg0J_WuTv"
      },
      "source": [
        "Now we have everything we need to implement an encoder layer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "16zvGFBo_uaQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, intermediate_size, drop_rate=0.1, LNepsilon=1e-12):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attention = MultiHeadAttention(hidden_size, num_heads)\n",
        "        self.ffn = FFN(intermediate_size, hidden_size, drop_rate)\n",
        "        self.addnorm1 = AddNorm(hidden_size, LNepsilon, drop_rate)  # Pass hidden_size\n",
        "        self.addnorm2 = AddNorm(hidden_size, LNepsilon, drop_rate)  # Pass hidden_size\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        att_out = self.attention(inputs, mask)\n",
        "        att_out = self.addnorm1(inputs, att_out)\n",
        "        ffn_out = self.ffn(att_out)\n",
        "        ffn_out = self.addnorm2(att_out, ffn_out)\n",
        "        return ffn_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "umQ878ho-6Hp"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLPlDTeRXefl"
      },
      "source": [
        "In the previous part, you implemented the encoder layer. We only need two more layers to implement BERT. First layer is the embedding layer. The final embedding for each token in BERT is the addition of three types of embeddings. Aside from token embeddings, there is also segment embeddings and position embeddings. For this assignment we are ignoring the segment embeddings since we only want to do single sentence classification! <br>\n",
        "Unlike the transformer, which uses fixed positional embeddings, BERT uses learned positional embeddings.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "\n",
        "![](https://github.com/iust-deep-learning/982/raw/master/static_files/assignments/asg04_assets/bert_emb.PNG)\n",
        "\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Note that layer normalization followed by dropout is applied to the final embeddings (after adding all the embeddings).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VCX-g4qjojYr"
      },
      "source": [
        "**Question**: What is segment embedding's functionality in BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4I0E6_AOozvd"
      },
      "source": [
        "Segment embedding delimits the inputs into its own segments and helps distinguish between different segments. It is a trainable embedding and therefore I suspect it helps the model recognize when segments start/end.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DTW-F4t9_x24"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------ BERT ------------------------------\n",
        "\n",
        "class BertEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, maxlen, hidden_size):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        self.TokEmb = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
        "        self.PosEmb = nn.Parameter(torch.randn(maxlen, hidden_size))\n",
        "        self.LN = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        tok_emb = self.TokEmb(inputs)\n",
        "        pos_emb = self.PosEmb[:inputs.shape[1], :]\n",
        "        emb = tok_emb + pos_emb\n",
        "        emb = self.LN(emb)\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kPjWqcH-ytQP"
      },
      "source": [
        "The last layer you need to implement is the \"pooler\". The pooler converts the hidden states of the last encoder layer (which is of shape **[batch_size, sequence_lenght, hidden_size]**) to a vector representation (which is of shape **[batch_size, hidden_size]**) for each input sentence. The pooler does this by simply taking the hidden state corresponding to the first token (a special token in the beggining of each sentence) and feeding it to a dense layer (tanh is used as the activation function of this dense layer in the original implementation). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "O719umhMz_UH"
      },
      "outputs": [],
      "source": [
        "class Pooler(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Pooler, self).__init__()\n",
        "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
        "        self.classifier = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        first_token = encoder_out[:, 0, :]\n",
        "        pooled_out = self.dense(first_token)\n",
        "        classified = self.classifier(pooled_out)\n",
        "        return classified\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5FsjGcFxo8JU"
      },
      "source": [
        "**Question**: As it was explained earlier, the pooler's job is to create a single vector representation of a sentence (or sentence pair) by taking the hidden state corresponding to the first token. Can you suggest another form of pooling that could work for BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QvR3jJIEpkka"
      },
      "source": [
        "First intuition would be to squeeze the hidden state of the last encoder layer to be of size [batch_size, sequence_length * hidden_size] and push it through a linear layer, with weights of dimension [sequence_length * hidden_size, hidden_size]. Then we would get a output of dim [batch_size, hidden_size]. But this would consider all of the tokens, not just the classifier token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-P8zt_tFojZY"
      },
      "source": [
        "Now you can use the the **create_BERT** function in the cell below. This function gets BERT's hyper-parameters as its inputs and return a BERT model. Use the functional api to create the model.<br>\n",
        "Note that the returned model must have two outputs (just like the pre-trained BERTs): \n",
        "- The hidden states of the last encoder layer\n",
        "- Output of the pooler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L9tD7UtNfZ4p"
      },
      "outputs": [],
      "source": [
        "def create_BERT(vocab_size, maxlen, hidden_size, num_layers, num_att_heads, intermediate_size, drop_rate=0.1):\n",
        "    \"\"\"\n",
        "    Creates a BERT model based on the arguments provided.\n",
        "\n",
        "    Arguments:\n",
        "    vocab_size: number of words in the vocabulary\n",
        "    maxlen: maximum length of each sentence\n",
        "    hidden_size: dimension of the hidden state of each encoder layer\n",
        "    num_layers: number of encoder layer\n",
        "    num_att_heads: number of attention heads in the multi-headed attention layer\n",
        "    intermediate_size: dimension of the intermediate layer in the feed-forward sublayer of the encoders\n",
        "    drop_rate: dropout rate of all the dropout layers used in the model\n",
        "    returns:\n",
        "    model: a BERT model\n",
        "    \"\"\"\n",
        "\n",
        "    emb = BertEmbedding(vocab_size, maxlen, hidden_size)\n",
        "    encoder_layers = nn.ModuleList([Encoder(hidden_size, num_att_heads, intermediate_size, drop_rate, LNepsilon=1e-12) for _ in range(num_layers)])  # Pass LNepsilon\n",
        "    pooler = Pooler(hidden_size)\n",
        "    classifier = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    model = nn.ModuleList([emb, encoder_layers, pooler, classifier])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PKBEBTI6sFKu"
      },
      "source": [
        "The Rotten tomatoes critic reviews dataset is used for this assignment. This dataset consists of about 350000 short reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IUn-48AVXbfR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/piragi/projects/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_reviews, test_reviews = pd.read_csv('train_reviews_small.csv').values[:, 1:], pd.read_csv('test_reviews_small.csv').values[:, 1:]\n",
        "\n",
        "(train_texts, train_labels), (test_texts, test_labels) = (train_reviews[:, 0], train_reviews[:, 1]), (test_reviews[:, 0], test_reviews[:, 1])\n",
        "\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "aprx_vocab_size = 20000\n",
        "cls_token = '[cls]'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Using pre-trained BERT tokenizer\n",
        "MAXLEN = 32\n",
        "\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded_text = tokenizer.encode_plus(text, add_special_tokens=True, max_length=MAXLEN, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        input_ids = encoded_text['input_ids'].squeeze()\n",
        "        attention_mask = encoded_text['attention_mask'].squeeze()\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': torch.tensor(label, dtype=torch.long)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UhTvBa9ntO7b"
      },
      "source": [
        "Now complete the **encode_sentence** function in the cell below. This function recieves a sentence and an integer denoting the maximum length of the sentence as inputs and returns a list of token ids. Here are the steps to implement this function:\n",
        "- encode the input sentence using the trained tokenizer to receive a token id list\n",
        "- zero-pad the token id list to the maximum length\n",
        "- add the id corresponding to the special token to the beggining of the token id list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JBirx1Fbvv-k"
      },
      "source": [
        "Now use the functional api and the **create_BERT** function you implemented earlier to create a classifier for the movie reviews dataset.\n",
        "Note that the intermediate layer in the feed-forward sub-layer of the encoders is set to $4\\times H$ in the original BERT implementation, where $H$ is the hidden layer size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SZOW4L9gBqvc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 0.6921, Accuracy = 0.6142\n",
            "Epoch 2: Loss = 0.6369, Accuracy = 0.6339\n",
            "Epoch 3: Loss = 0.6018, Accuracy = 0.6328\n",
            "Epoch 4: Loss = 0.5712, Accuracy = 0.6943\n",
            "Epoch 5: Loss = 0.5326, Accuracy = 0.6827\n",
            "Epoch 6: Loss = 0.4918, Accuracy = 0.6933\n",
            "Epoch 7: Loss = 0.4347, Accuracy = 0.6819\n",
            "Epoch 8: Loss = 0.3853, Accuracy = 0.6905\n",
            "Epoch 9: Loss = 0.3197, Accuracy = 0.6621\n",
            "Epoch 10: Loss = 0.2626, Accuracy = 0.6823\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------ Training ------------------------------\n",
        "\n",
        "train_dataset = ReviewsDataset(train_texts, train_labels)\n",
        "test_dataset = ReviewsDataset(test_texts, test_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Using pre-trained BERT-Base hyperparameters\n",
        "hidden_size = 768\n",
        "num_heads = 6\n",
        "num_layers = 6\n",
        "\n",
        "# Creating BERT model\n",
        "model = create_BERT(vocab_size=tokenizer.vocab_size, maxlen=MAXLEN, hidden_size=hidden_size, num_layers=num_layers, num_att_heads=num_heads, intermediate_size=4*hidden_size)\n",
        "\n",
        "# Defining optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "# use tqdm for progress bar\n",
        "batch_number = 0\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch_number += 1\n",
        "        #print(f'Batch % completed : {batch_number/len(train_dataloader)}')\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device).float().view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass input through the model\n",
        "        emb = model[0](input_ids)\n",
        "        for encoder in model[1]:\n",
        "            emb = encoder(emb, attention_mask)\n",
        "        output = model[2](emb)\n",
        "        # Calculate loss\n",
        "        loss = loss_fn(output, labels)\n",
        "\n",
        "        # Backpropagate and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del input_ids, attention_mask, labels, emb, output, loss\n",
        "    \n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].float().to(device).view(-1, 1)\n",
        "        with torch.no_grad():\n",
        "            emb = model[0](input_ids)\n",
        "            for encoder in model[1]:\n",
        "                emb = encoder(emb, attention_mask)\n",
        "            output = model[2](emb)\n",
        "\n",
        "            probs = torch.sigmoid(output)\n",
        "            predicted_labels = (probs > 0.5).float()  # Threshold probabilities to get binary predictions\n",
        "            correct += (predicted_labels == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Epoch {epoch+1}: Loss = {total_loss / len(train_dataloader):.4f}, Accuracy = {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure that model and dataloader are not storing unnecessary information\n",
        "del model, train_dataloader, test_dataloader\n",
        "torch.cuda.empty_cache()  # Final clearance after training is complete\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   9294 MiB |   9486 MiB | 374044 MiB | 364749 MiB |\n",
            "|       from large pool |   9291 MiB |   9483 MiB | 373177 MiB | 363886 MiB |\n",
            "|       from small pool |      3 MiB |      6 MiB |    867 MiB |    863 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   9294 MiB |   9486 MiB | 374044 MiB | 364749 MiB |\n",
            "|       from large pool |   9291 MiB |   9483 MiB | 373177 MiB | 363886 MiB |\n",
            "|       from small pool |      3 MiB |      6 MiB |    867 MiB |    863 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |   9290 MiB |   9482 MiB | 373950 MiB | 364659 MiB |\n",
            "|       from large pool |   9286 MiB |   9478 MiB | 373083 MiB | 363796 MiB |\n",
            "|       from small pool |      3 MiB |      6 MiB |    866 MiB |    862 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   9582 MiB |   9582 MiB |  13548 MiB |   3966 MiB |\n",
            "|       from large pool |   9578 MiB |   9578 MiB |  13538 MiB |   3960 MiB |\n",
            "|       from small pool |      4 MiB |      8 MiB |     10 MiB |      6 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  97445 KiB | 354087 KiB |  61404 MiB |  61309 MiB |\n",
            "|       from large pool |  97270 KiB | 352246 KiB |  60528 MiB |  60433 MiB |\n",
            "|       from small pool |    175 KiB |   3348 KiB |    876 MiB |    875 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     683    |     768    |   15420    |   14737    |\n",
            "|       from large pool |     369    |     371    |    9961    |    9592    |\n",
            "|       from small pool |     314    |     420    |    5459    |    5145    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     683    |     768    |   15420    |   14737    |\n",
            "|       from large pool |     369    |     371    |    9961    |    9592    |\n",
            "|       from small pool |     314    |     420    |    5459    |    5145    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     205    |     207    |     279    |      74    |\n",
            "|       from large pool |     203    |     204    |     274    |      71    |\n",
            "|       from small pool |       2    |       4    |       5    |       3    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      68    |    3882    |    3840    |\n",
            "|       from large pool |      37    |      59    |    2390    |    2353    |\n",
            "|       from small pool |       5    |      15    |    1492    |    1487    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "assignment_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
